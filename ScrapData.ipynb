{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ScrapData.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPD85j2HtIRiRfC+ciCeirp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hoangthang017/CS114.K21/blob/master/ScrapData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcTgIzsuBCT2",
        "colab_type": "text"
      },
      "source": [
        "# **Scrap dữ liệu từ 2 trang web :** \n",
        "  - HuffPost : non-sarcastic\n",
        "  - TheOnion : sarcastic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s18RpnlCofM",
        "colab_type": "text"
      },
      "source": [
        "####**import thư viện :**\n",
        "  - Request : gửi yêu cầu đến url\n",
        "  - urlopen : mở url\n",
        "  - BeautifulSoup : lấy headline từ các thẻ trong source của url"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHcjrJhSwZ1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import Request , urlopen \n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wtu5j1WHWn4",
        "colab_type": "text"
      },
      "source": [
        "####**Scrap dữ liệu từ trang HuffPost**\n",
        "- Ý tưởng : Scrap dữ liệu từng tab trong mục News của trang HuffPost UK\n",
        "- Thực hiện :\n",
        "  - Sử dụng vòng for để kiểm soát số tab cần scrap để tránh dữ liệu quá ít hoặc quá nhiều\n",
        "  - Sau khi tìm được url của website cần scrap thì ta sẽ gửi yêu cầu đến url đó \n",
        "  - Tiếp đến mở url vừa được yêu cầu\n",
        "  - Sử dụng BeautifullSoup để lấy source của website vừa mới được mở \n",
        "  - Một đoạn code chứa headline : **Judge Rejects White House Demand To Block John Bolton's New Book**\n",
        "\n",
        "      ```<a class=\"card__link yr-card-headline\" href=\"/entry/john-bolton-white-house-trump-book_uk_5eee269bc5b697d43e591553?utm_hp_ref=uk-news\" target=\"_self\" data-cpid=\"0d6874cb-8dbc-355e-89b3-76c1cbedf86a\" data-rapid-pos=\"2\" data-ylk=\"cpos:18;pos:2;elm:hdln;g:0d6874cb-8dbc-355e-89b3-76c1cbedf86a\" data-rapid-sec=\"{&quot;col2&quot;:&quot;col2&quot;}\" data-rapid-elm=\"hdln\" data-rapid-g=\"0d6874cb-8dbc-355e-89b3-76c1cbedf86a\" data-rapid_p=\"37\" data-v9y=\"1\">Judge Rejects White House Demand To Block John Bolton's New Book</a>```\n",
        "\n",
        "  - Sau khi lấy được source của web, ta quan sát thấy các headline của trang này nằm trong thẻ <\\a> với class_name = card__link yr-card-headline \n",
        "  - Vì thế ta dùng hàm find_all của thư viện BeautifullSoup để lấy ra tất các thẻ này\n",
        "  - Tiếp theo ta tách headline bằng hàm text \n",
        "  - Cuối cùng kiểm tra xem headline vừa mới lấy ra có nằm trong Data chưa , nếu chưa thì đẩy nó và label của dữ liệu này là 0 vào data\n",
        "- Kết quả : từ 50 tab của mục News ta lấy được 1600 headline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuyWDyVuIcrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scrap data form HuffPost\n",
        "huff_data = []\n",
        "\n",
        "# sử dụng vòng lặp để load trang tiếp theo trong phần news \n",
        "for i in range(50):\n",
        "\n",
        "  # định dạng url của phần news trong trang HuffPost với i tương ứng với số trang\n",
        "  url = 'https://www.huffingtonpost.co.uk/news/'+ str(i) +'/?guccounter=2'\n",
        "\n",
        "  # gửi request HTTP 1.1 đến url với header tự chỉnh \n",
        "  req = Request(url,headers={'User-Agent': 'ScrapData/3.0'})\n",
        "\n",
        "  # mở url với thời gian chờ là 10s \n",
        "  page = urlopen(req,timeout=10)\n",
        "\n",
        "  # sử dụng BeautifulSoup để lấy source của website với định dạng văn bản html\n",
        "  soup = BeautifulSoup(page,\"html.parser\")\n",
        "\n",
        "  # sử dụng hàm find_all để tìm tất cả thẻ a có name_class : card__link yr-card-headline\n",
        "  headlines = soup.find_all('a',attrs={'class':'card__link yr-card-headline'})\n",
        "\n",
        "  # tách headline từ thẻ và gán label cho nó \n",
        "  for hl in range(len(headlines)):\n",
        "\n",
        "    # hàm text sẽ lấy ra phần text trong thẻ \n",
        "    headline = headlines[hl].text\n",
        "\n",
        "    # kiểm tra headline có tồn tại trong data chưa \n",
        "    if headline not in huff_data:\n",
        "      data = [headline,0]\n",
        "      huff_data.append(data)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP-KokwOK8wP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a719d53b-487a-45d2-a044-5ca360aa9dfc"
      },
      "source": [
        "print(huff_data[:10])\n",
        "print(len(huff_data))"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[\"The White House's Attempt To Explain Trump's Tiny Rally Turnout Is As Nonsensical As You'd Expect\", 0], ['Before Covid-19 This Was Going To Be My First Black Pride', 0], ['Just Spit Into A Pot For New Coronavirus Test That Could Replace Throat Swab', 0], [\"Reading Park Terror Suspect 'Came To The Attention Of MI5 Last Year'\", 0], ['Campaigners Blast Police As Black Driver Is Pulled Over On Suspicion Of Using Vaseline To Smuggle Drugs', 0], ['Manchester Shooting: Double Murder Inquiry Launched As Second Victim Dies', 0], ['1 Dead, 11 Injured In Minneapolis Shooting', 0], ['Reading Stabbing Attack Declared A Terrorist Incident, Police Say', 0], [\"Law Graduate Pursues Private Prosecution Of 'Arrogant' Dominic Cummings\", 0], [\"Trump Uses Racist Terms 'Kung Flu' And 'Chinese Virus' To Describe Covid-19\", 0]]\n",
            "1600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJPKT7bYOlQe",
        "colab_type": "text"
      },
      "source": [
        "####**Scrap dữ liệu từ trang TheOnion**\n",
        "- Ý tưởng : tương tự trang HuffPost lấy dữ liệu của từng tab của mục News-in-Brief\n",
        "- Thực hiện : \n",
        "   - Khá giống với scrap dữ liệu của trang HuffPost\n",
        "   - Tuy nhiên phần url của tab mới có dạng url gốc + 1 chuỗi ứng với tab đó \n",
        "   - Code của một More-Stroies button:\n",
        "\n",
        "    ```<a class=\"sc-1out364-0 hMndXN js_link\" data-ga=\"[[&quot;Story type page click&quot;,&quot;More stories click&quot;]]\" href=\"?startTime=1592331480884\" rel=\"next\"><button class=\"j48i5d-2 fUFqcv button button--tertiary\"><div class=\"j48i5d-1 fuDdQ\"><span class=\"j48i5d-0 keLEgo\">More stories</span><span class=\"iyvn34-0 bYIjtl\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"18\" height=\"18\" aria-label=\"ChevronRight icon\" viewBox=\"0 0 18 18\"><path fill-rule=\"evenodd\" d=\"M6.16 14.13a.5.5 0 1 0 .68.74l6-5.5a.5.5 0 0 0 0-.74l-6-5.5a.5.5 0 1 0-.68.74L11.76 9l-5.6 5.13z\"></path></svg></span></div></button></a>```\n",
        "\n",
        "   - Từ đây có thể thấy tail_url tab tiếp theo nằm trong href (?startTime=1592331480884)\n",
        "   - Ta có thể tìm thông qua thẻ a và class_name = sc-1out364-0 hMndXN js_link tuy nhiên khi quan sát source có rất nhiều thẻ với class_name như vậy\n",
        "   - Vì thế ta sẽ tìm thông qua data-ga=\"[[&quot;Story type page click&quot;,&quot;More stories click&quot;]]\"\n",
        "   -Từ đó có thể dùng hàm get('href') để lấy tail_url\n",
        "   - Kiểm tra và đẩy dữ liệu vào data với label = 1\n",
        "- Kết quả : sau khi scrap 80 tab ta cũng thu được 1600 headline \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPjaNGMvP1Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scrap dữ liệu từ trang onion\n",
        "onion_data= []\n",
        "\n",
        "# đuôi url của mỗi tab \n",
        "tail_url = ''\n",
        "\n",
        "# Sử dụng vòng lặp để scrap từng tab\n",
        "for i in range(80):\n",
        "  # url có định dạng url gốc của mục news-in-brief cộng với tail_url của mỗi tab, tab đầu tiên có đuôi trong từ tab 2 đuôi url sẽ ứng với 1 chuỗi số \n",
        "  url = 'https://www.theonion.com/c/news-in-brief' + tail_url\n",
        "\n",
        "  # gửi yêu cầu đến url\n",
        "  req = Request(url, headers={'ScrapData':'XYZ/3.0'})\n",
        "\n",
        "  # mở url\n",
        "  page = urlopen(req,timeout=10)\n",
        "\n",
        "  # lấy source của web\n",
        "  soup = BeautifulSoup(page,'html.parser')\n",
        "\n",
        "  # tìm tất cả thẻ h2 với class_name = sc-759qgu-0 cYlVdn cw4lnv-6 eXwNRE\n",
        "  headlines = soup.find_all('h2',attrs={'class':'sc-759qgu-0 cYlVdn cw4lnv-6 eXwNRE'})\n",
        "\n",
        "  # tách headline và gắn label\n",
        "  for hl in range(len(headlines)):\n",
        "\n",
        "    # tách headline \n",
        "    headline = headlines[hl].text\n",
        "\n",
        "    # kiểm tra headline có tồn tại trong data chưa \n",
        "    if headline not in onion_data:\n",
        "      data = [headline,1]\n",
        "      onion_data.append(data)\n",
        "\n",
        "  # lấy tail_url của tab tiếp theo nằm trong href của thẻ a với data-ga = [[\"Story type page click\",\"More stories click\"]]\n",
        "  # sử dụng hàm get('href') để lấy dữ liệu của href \n",
        "  tail_url = str(soup.find('a',attrs={'data-ga':'[[\"Story type page click\",\"More stories click\"]]'}).get('href'))"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPpSOMW6SYZp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f16acd17-a53c-4bc3-ef8f-c1d7dab7f8eb"
      },
      "source": [
        "print(onion_data[:10])\n",
        "print(len(onion_data))"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Dog Owner Not Sure How City Expects Her To Pick Up Every Drop Of Dog Piss In Little Bag', 1], ['Public Pressure Mounts For U.S. Government To Stop Designating KKK As 501(c)(3) Nonprofit', 1], ['Huh, Boyfriend’s Ex Just Made Interesting Hair Choice', 1], ['‘It’s Perfect Outside,’ Announces Sweating Woman Slowly Losing Consciousness In Middle Of Heatstroke', 1], ['Man Hates How Hot Dog-Eating Contests Reduce Art Of Eating Hot Dogs To A Competition', 1], ['Resigned Pew Research Study Has No Fucking Clue What’s Going On With 15% Of Americans', 1], ['White House Toilet Doesn’t Know If It Can Handle Another 4 Years Of Trump', 1], ['Kentucky Attorney General So Starstruck By Letter From Beyoncé He Unable To Even Read What It’s About', 1], ['Researchers Find Crows Smart Enough Not To Let On How Smart They Really Are', 1], ['Sony Reveals PS5 Fully Customizable With Different Little Hats', 1]]\n",
            "1600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtCGQ5dqTGj9",
        "colab_type": "text"
      },
      "source": [
        "#**Ghi dữ liệu vào file CSV**\n",
        "- Mount đến drive để lấy tệp cần ghi vào\n",
        "- Dùng thư viện csv để chuyển data từ dạng list sang csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV2HIyryTcr1",
        "colab_type": "text"
      },
      "source": [
        "####**Mount drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myRxDNmG7N3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d79a6582-e2f1-4920-e68f-9ec773422625"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egGogB6QTjvl",
        "colab_type": "text"
      },
      "source": [
        "####**Chuyển từ List sang CSV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXIQW-g-7OME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# impor thư viện csv\n",
        "import csv\n",
        "\n",
        "# các header của dữ liệu\n",
        "Headers = [[ \"Headline\", \"label\"]]\n",
        "\n",
        "# mở tệp Headlines_Data trong Drive với chế độ ghi\n",
        "with open('/content/drive/My Drive/CS114/DataHeadline/Headlines_Data.csv', 'w', newline='') as file:\n",
        "  # Sử dụng hàm writer để thực hiện ghi vào file   \n",
        "    writer = csv.writer(file)\n",
        "\n",
        "  # sử dụng wrirerows để ghi thành nhiều cột \n",
        "    # ghi header\n",
        "    writer.writerows(row_list)\n",
        "\n",
        "    # ghi onion data\n",
        "    writer.writerows(onion_data)\n",
        "\n",
        "    # ghi huffpost data\n",
        "    writer.writerows(huff_data)"
      ],
      "execution_count": 124,
      "outputs": []
    }
  ]
}